{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# We need to download the 'punkt' package to use tokenizers\n",
    "nltk.download('punkt', download_dir='/tmp/')\n",
    "nltk.data.path.append(\"tmp\")\n",
    "\n",
    "sentence = \"I am not going to work today.\"\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags\n",
    "We can tag words using NLTK's pos_tag() function. Here is the list of tags:\n",
    "* CC\tcoordinating conjunction\n",
    "* CD\tcardinal digit\n",
    "* DT\tdeterminer\n",
    "* EX\texistential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "* FW\tforeign word\n",
    "* IN\tpreposition/subordinating conjunction\n",
    "* JJ\tadjective\t'big'\n",
    "* JJR\tadjective, comparative\t'bigger'\n",
    "* JJS\tadjective, superlative\t'biggest'\n",
    "* LS\tlist marker\t1)\n",
    "* MD\tmodal\tcould, will\n",
    "* NN\tnoun, singular 'desk'\n",
    "* NNS\tnoun plural\t'desks'\n",
    "* NNP\tproper noun, singular\t'Harrison'\n",
    "* NNPS\tproper noun, plural\t'Americans'\n",
    "* PDT\tpredeterminer\t'all the kids'\n",
    "* POS\tpossessive ending\tparent\\'s\n",
    "* PRP\tpersonal pronoun\tI, he, she\n",
    "* PRP$\tpossessive pronoun\tmy, his, hers\n",
    "* RB\tadverb\tvery, silently,\n",
    "* RBR\tadverb, comparative\tbetter\n",
    "* RBS\tadverb, superlative\tbest\n",
    "* RP\tparticle\tgive up\n",
    "* TO\tto\tgo 'to' the store.\n",
    "* UH\tinterjection\terrrrrrrrm\n",
    "* VB\tverb, base form\ttake\n",
    "* VBD\tverb, past tense\ttook\n",
    "* VBG\tverb, gerund/present participle\ttaking\n",
    "* VBN\tverb, past participle\ttaken\n",
    "* VBP\tverb, sing. present, non-3d\ttake\n",
    "* VBZ\tverb, 3rd person sing. present\ttakes\n",
    "* WDT\twh-determiner\twhich\n",
    "* WP\twh-pronoun\twho, what\n",
    "* WP$\tpossessive wh-pronoun\twhose\n",
    "* WRB\twh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger', download_dir='/tmp/')\n",
    "\n",
    "text = word_tokenize(\"They refuse to permit us to obtain the refuse permit\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "We can also use some stop words that we can remove from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords', download_dir='/tmp/')\n",
    "\n",
    "sentence = \"This is a sample sentence, let's test the stopword filtretation.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the list of stop words in NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming:\n",
    "We can use NLTK's stemming library for Stemming. Stemmer converts the tokens to simpler forms by removing the end part of tokens (pluralizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = [\"eat\", \"eats\", \"eating\"]\n",
    "ps = PorterStemmer()\n",
    " \n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use stemmer with a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# We need to download the 'punkt' package to use tokenizers\n",
    "nltk.download('punkt', download_dir='/tmp/')\n",
    "nltk.data.path.append(\"tmp\")\n",
    "\n",
    "ps = PorterStemmer()\n",
    " \n",
    "sentence = \"the children are playing and running. The weather was better yesterday.\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "for word in words:\n",
    "    print(word + \":\" + ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "We can use NLTK's lemma library for Lemmatization. Lemmatizer uses dictionary root to map the tokens. It also works better when the token position tag (\"noun\", \"verb\", etc. ) is provided.The first example is without word tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the wordnet package\n",
    "nltk.download('wordnet', download_dir='/tmp/')\n",
    "nltk.data.path.append(\"tmp\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"the children are playing and running. the weather was better yesterday.\"\n",
    "words = word_tokenize(sentence)\n",
    " \n",
    "for word in words:\n",
    "    print(word + \":\" + wl.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the token position tags this time. Lemmatizer does a better job when we use the tag information that is also calculated using NLTK's postition tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download the tagging package\n",
    "nltk.download('averaged_perceptron_tagger', download_dir='/tmp/')\n",
    "nltk.data.path.append(\"tmp\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "wl = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "        \n",
    "sentence = \"the children are playing and running. the weather was better yesterday.\"\n",
    "words = word_tokenize(sentence)\n",
    "word_pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "print(\"Tags\", word_pos_tags)\n",
    "\n",
    "for idx, tag in enumerate(word_pos_tags):\n",
    "    print(words[idx], wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
